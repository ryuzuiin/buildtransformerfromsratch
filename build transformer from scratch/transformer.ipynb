{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e897bbb5-3689-41c9-a995-989b8d2e3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor, BoolTensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ea278-1617-4949-a6b8-c8917bdc9e92",
   "metadata": {},
   "source": [
    "# Building the attention part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43f569-136d-410d-adb4-dee856ec6184",
   "metadata": {},
   "source": [
    "### Key point: \n",
    "\n",
    "attention allows modern neural networks to focus on the most relevant pieces of the input whether text, images, or multimodal inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bce3c6-f0f0-497a-a7b7-6365114e751b",
   "metadata": {},
   "source": [
    "### Input embedding:\n",
    "\n",
    "After word2vec, tokens are structed. \n",
    "embbeding_size = position_embedding, token vector = word embedding + potion embedding.\n",
    "token vector -> linear layer -> hidden_size\n",
    "more hidden size, more information, more parameters, more cost.Normally, = 512, 768, 1024\n",
    "multi_ head optimizes hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3e19b2-a005-4e38-9a37-662e13e173ca",
   "metadata": {},
   "source": [
    "## Single Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ba23f4a-8104-451e-ac74-d46a9d6fb584",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear layer for transforming input tensor to query, key, and value tensors\n",
    "        #qkv_projected = input_tensor @ qkv_projection.weight^T + qkv_projection.bias\n",
    "        #qkv_projection.weigh.shape = (hidden_size, (hidden_size // 4) * 3)\n",
    "        self.qkv_projection = nn.Linear(hidden_size, (hidden_size // 4) * 3, bias=bias)\n",
    "        \n",
    "        # Linear layer for final output projection\n",
    "        self.output_projection = nn.Linear(hidden_size // 4, hidden_size, bias=bias)\n",
    "\n",
    "    def forward(self, input_tensor: Tensor):\n",
    "        batch_size, sequence_length, hidden_size = input_tensor.shape\n",
    "        \n",
    "        # Project input tensor to query, key, and value tensors\n",
    "        qkv_projected = self.qkv_projection(input_tensor)\n",
    "        qkv_projected = qkv_projected.reshape(batch_size, sequence_length, 3, hidden_size // 4)\n",
    "        q, k, v = qkv_projected.unbind(dim=2)\n",
    "        \n",
    "        # Compute attention weights using query and key tensors\n",
    "        attention_weights = q @ k.transpose(-2, -1)\n",
    "        attention_weights = attention_weights / torch.sqrt(torch.tensor(k.size(-1)))\n",
    "        attention_weights = torch.softmax(attention_weights, dim=-1)\n",
    "        \n",
    "        # Apply attention weights to value tensor\n",
    "        attended_values = attention_weights @ v\n",
    "        \n",
    "        # Project attended values to final output\n",
    "        output_tensor = self.output_projection(attended_values)\n",
    "        \n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "#Q = input @ W_Q\n",
    "#K = input @ W_K\n",
    "#V = input @ W_V\n",
    "#Attention_Weights = softmax(Q @ K^T / sqrt(d_k))\n",
    "#Output = Attention_Weights @ V\n",
    "\n",
    "#两个矩阵相乘，可以为注意力引入2次，引入一些非线性(not sure)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db0890c-2757-4f25-bc23-bc40c5c22bc7",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc2b47f-0111-429e-acdb-5f2d21c024d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, dropout: float = 0.1, bias: bool = True):\n",
    "        \"\"\"\n",
    "        Initialization function for the Multi-Head Attention module.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): Size of the input and output hidden layers.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Probability of dropout. Default is 0.1.\n",
    "            bias (bool): Whether to use bias in the linear layers. Default is True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"Hidden size must be divisible by the number of heads\"\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Linear layer for computing Q, K, V\n",
    "        self.qkv_linear = nn.Linear(hidden_size, hidden_size * 3, bias=bias)\n",
    "        # Linear layer for computing the final output\n",
    "        self.output_linear = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward propagation function for the Multi-Head Attention module.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, hidden_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, seq_len, hidden_size).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Compute Q, K, V\n",
    "        qkv = self.qkv_linear(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, batch_size, num_heads, seq_len, head_dim)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        attn_weights = attn_weights / math.sqrt(self.head_dim)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_output = torch.matmul(attn_weights, v)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        attn_output = attn_output.transpose(1, 2)  # (batch_size, seq_len, num_heads, head_dim)\n",
    "        attn_output = attn_output.reshape(batch_size, seq_len, self.hidden_size)\n",
    "\n",
    "        # Compute final output\n",
    "        output = self.output_linear(attn_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "#dropout both the attention weights and the final layer, with a default dropout probability of 10 percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c06d0-f111-46c7-a887-4e62e26be476",
   "metadata": {},
   "source": [
    "## Bidirectional  Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd613c5a-164d-480e-8726-9bb8420d5d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, dropout: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"Hidden size must be divisible by the number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor, mask: BoolTensor = None) -> Tensor:\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "\n",
    "        q = self.q_proj(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        attn_scores = torch.einsum(\"bqhd,bkhd->bhqk\", [q, k]) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "\n",
    "        attn_probs = self.attn_dropout(F.softmax(attn_scores, dim=-1))\n",
    "        attn_output = torch.einsum(\"bhqv,bqhd->bqhd\", [attn_probs, v])\n",
    "        attn_output = attn_output.contiguous().view(batch_size, seq_length, hidden_size)\n",
    "        output = self.proj_dropout(self.out_proj(attn_output))\n",
    "\n",
    "        return output\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af02f49a-e6eb-4ece-a06c-bbd7d5ebe878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, context_size: int, dropout: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"Hidden size must be divisible by the number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(context_size, context_size, dtype=torch.bool), diagonal=1))\n",
    "\n",
    "    def forward(self, x: Tensor, mask: BoolTensor = None) -> Tensor:\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "\n",
    "        q = self.q_proj(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        k = self.k_proj(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        v = self.v_proj(x).view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        attn_scores = torch.einsum(\"bqhd,bkhd->bhqk\", [q, k]) / math.sqrt(self.head_dim)\n",
    "\n",
    "        causal_mask = self.causal_mask[:seq_length, :seq_length].unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(causal_mask | mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_scores = attn_scores.masked_fill(causal_mask, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = self.dropout(F.softmax(attn_scores, dim=-1))\n",
    "        attn_output = torch.einsum(\"bhqv,bqhd->bqhd\", [attn_probs, v])\n",
    "        attn_output = attn_output.contiguous().view(batch_size, seq_length, hidden_size)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c817b71c-99e1-46aa-a2c8-780422e758cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalCrossAttention(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_heads: int, context_size: int, dropout: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0, \"Hidden size must be divisible by the number of heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.kv_proj = nn.Linear(hidden_size, hidden_size * 2, bias=bias)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(context_size, context_size, dtype=torch.bool), diagonal=1))\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor, mask: BoolTensor = None) -> Tensor:\n",
    "        batch_size, seq_length, hidden_size = x.size()\n",
    "\n",
    "        q = self.q_proj(x).view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k, v = self.kv_proj(y).view(batch_size, seq_length, 2, self.num_heads, self.head_dim).unbind(dim=2)\n",
    "\n",
    "        attn_scores = torch.einsum(\"bnqd,bnkd->bnqk\", [q, k]) / math.sqrt(self.head_dim)\n",
    "\n",
    "        causal_mask = self.causal_mask[:seq_length, :seq_length].unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(causal_mask | mask.unsqueeze(1).unsqueeze(1), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_scores = attn_scores.masked_fill(causal_mask, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = self.dropout(F.softmax(attn_scores, dim=-1))\n",
    "        attn_output = torch.einsum(\"bnqv,bnqd->bnqd\", [attn_probs, v]) #change the order\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_length, hidden_size)\n",
    "        output = self.out_proj(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794687c2-9eaa-45cf-8a10-435fc0b8e3c8",
   "metadata": {},
   "source": [
    "## Feed Forward Network\n",
    "\n",
    "two takeaways: 1. polysemantic; 2. superposition hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d578bb-2b02-4f44-9aac-aa6c53561fc1",
   "metadata": {},
   "source": [
    "FNN = Act(XW1)W2\n",
    "\n",
    "FNN operates on each token independently of all other tokens in the sequance.\n",
    "\n",
    "It cannot reference other tokens or positional information outside of the information embeded in the current token vecto\n",
    "\n",
    "X --> high dimension  --> original dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33babac-4388-4a7a-818b-fd378c41168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size: int, expand_size: int, dropout: float = 0.1, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # Project input to expanded dimension\n",
    "        self.input_projection = nn.Linear(hidden_size, expand_size, bias=bias)\n",
    "        # Activation function to introduce non-linearity\n",
    "        self.activation = nn.GELU()\n",
    "        # Project back to the input dimension\n",
    "        self.output_projection = nn.Linear(expand_size, hidden_size, bias=bias)\n",
    "        # Optional dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Apply input projection\n",
    "        output = self.input_projection(x)\n",
    "        # Apply activation function\n",
    "        output = self.activation(output)\n",
    "        # Apply output projection\n",
    "        output = self.output_projection(output)\n",
    "        # Optionally apply dropout layer\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd95a26-bc67-40a5-be1e-055ee864b7f0",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de2752-255b-432d-b34b-b8813320b7cb",
   "metadata": {},
   "source": [
    "Post-Norm can suffer from gradient vanishing as normalization is applied to the outputs of initial layers multiple times.\n",
    "\n",
    "this can cause the gradient norm to become exponentially small which hinders model training.Using small learning rates and learning rate warmup improves Post-Norm training.\n",
    "\n",
    "pre-Norm applies the normalization layer to the input before it's passed to the Attention and Feed Forward layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c0e15f-6c27-472d-b518-de28d86ee0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        context_size: int,\n",
    "        expand_size: int,\n",
    "        attention: nn.Module = CausalAttention,\n",
    "        dropout: float = 0.1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Layer normalization before attention\n",
    "        self.attn_norm = nn.LayerNorm(hidden_size)\n",
    "        # Attention layer\n",
    "        self.attn = attention(\n",
    "            hidden_size=hidden_size,\n",
    "            num_heads=num_heads,\n",
    "            context_size=context_size,\n",
    "            dropout=dropout,\n",
    "            bias=bias,\n",
    "        )\n",
    "        # Layer normalization before feed-forward, pre-norm\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size)\n",
    "        # Feed-forward layer\n",
    "        self.ffn = FeedForward(\n",
    "            hidden_size=hidden_size,\n",
    "            expand_size=expand_size,\n",
    "            dropout=dropout,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # Residual connection for attention\n",
    "        attn_output = self.attn_norm(input)\n",
    "        attn_output = input + self.attn(attn_output)\n",
    "\n",
    "        # Residual connection for feed-forward\n",
    "        ffn_output = self.ffn_norm(attn_output)\n",
    "        ffn_output = attn_output + self.ffn(ffn_output)\n",
    "\n",
    "        return ffn_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
